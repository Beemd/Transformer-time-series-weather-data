{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "time_series_transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzP9p5MAVd8j"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAqFU8mBVufR",
        "outputId": "4b69b2d9-56fa-466b-d305-8767b22b410f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip\n",
            "13574144/13568290 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT_p0eUaVyW2"
      },
      "source": [
        "#weather data collected after every 10 minutes. \n",
        "df = pd.read_csv(csv_path)\n",
        "#We want to predict the weather after every hour\n",
        "# slice [start:stop:step], starting from index 5 take every 6th record\n",
        "df = df[5::6]\n",
        "\n",
        "date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAnAKuxGaZ4m",
        "outputId": "b9054a4a-6b07-41c7-ef9a-57634b1cb46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#replacing erroneous values in the wind vecolity data, like -9999 with 0\n",
        "#min wind velocity\n",
        "wv = df['wv (m/s)']\n",
        "bad_wv = wv == -9999.0\n",
        "wv[bad_wv] = 0.0\n",
        "\n",
        "#max wind velocity\n",
        "max_wv = df['max. wv (m/s)']\n",
        "bad_max_wv = max_wv == -9999.0\n",
        "max_wv[bad_max_wv] = 0.0\n",
        "\n",
        "# The above inplace edits are reflected in the DataFrame\n",
        "df['wv (m/s)'].min()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeF8m7hdahRB"
      },
      "source": [
        "#convert wind direction and velocity columns to a wind vector for the model to interpret easily\n",
        "wv = df.pop('wv (m/s)')\n",
        "max_wv = df.pop('max. wv (m/s)')\n",
        "\n",
        "# Convert to radians\n",
        "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
        "\n",
        "# Calculate the wind x and y components\n",
        "df['Wx'] = wv*np.cos(wd_rad)\n",
        "df['Wy'] = wv*np.sin(wd_rad)\n",
        "\n",
        "# Calculate the max wind x and y components\n",
        "df['max Wx'] = max_wv*np.cos(wd_rad)\n",
        "df['max Wy'] = max_wv*np.sin(wd_rad)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvDKKsK-b_cd"
      },
      "source": [
        "#converting date-time into seconds\n",
        "timestamp_s = date_time.map(datetime.datetime.timestamp)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZFYAsTLcCOX"
      },
      "source": [
        "#converting the date-time in seconds into sinusoidal function using sine and cosine\n",
        "day = 24*60*60\n",
        "year = (365.2425)*day\n",
        "\n",
        "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJxRrDOWcIzU"
      },
      "source": [
        "#split data into training, validation and test (70%,20%,10%)\n",
        "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
        "\n",
        "n = len(df)\n",
        "train_df = df[0:int(n*0.7)]\n",
        "val_df = df[int(n*0.7):int(n*0.9)]\n",
        "test_df = df[int(n*0.9):]\n",
        "\n",
        "num_features = df.shape[1]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyQGnNGocKt-"
      },
      "source": [
        "#normalizing data by subtracting the mean and dividing by standard deviation.\n",
        "#mean and standard deviation are calculated using training data set only.\n",
        "train_mean = train_df.mean()\n",
        "train_std = train_df.std()\n",
        "\n",
        "train_df = (train_df - train_mean) / train_std\n",
        "val_df = (val_df - train_mean) / train_std\n",
        "test_df = (test_df - train_mean) / train_std"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFJe4y4GcPlw"
      },
      "source": [
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift,\n",
        "               train_df=train_df, val_df=val_df, test_df=test_df,\n",
        "               label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = train_df\n",
        "    self.val_df = val_df\n",
        "    self.test_df = test_df\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(train_df.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])\n",
        "\n",
        "\n",
        "#separate windows for inputs and labels\n",
        "def split_window(self, features):\n",
        "  inputs = features[:, self.input_slice, :]\n",
        "  labels = features[:, self.labels_slice, :]\n",
        "  if self.label_columns is not None:\n",
        "    labels = tf.stack(\n",
        "        [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "        axis=-1)\n",
        "\n",
        "  # Slicing doesn't preserve static shape information, so set the shapes manually\n",
        "  inputs.set_shape([None, self.input_width, None])\n",
        "  labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "WindowGenerator.split_window = split_window\n",
        "\n",
        "\n",
        "#converting data into input_window and label_window pairs\n",
        "def make_dataset(self, data):\n",
        "  data = np.array(data, dtype=np.float32)\n",
        "  ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "      data=data,\n",
        "      targets=None,\n",
        "      sequence_length=self.total_window_size,\n",
        "      sequence_stride=1,\n",
        "      shuffle=True,\n",
        "      batch_size=64,)\n",
        "\n",
        "  ds = ds.map(self.split_window)\n",
        "\n",
        "  return ds\n",
        "\n",
        "WindowGenerator.make_dataset = make_dataset\n",
        "\n",
        "\n",
        "@property\n",
        "def train(self):\n",
        "  return self.make_dataset(self.train_df)\n",
        "\n",
        "@property\n",
        "def val(self):\n",
        "  return self.make_dataset(self.val_df)\n",
        "\n",
        "@property\n",
        "def test(self):\n",
        "  return self.make_dataset(self.test_df)\n",
        "\n",
        "\n",
        "WindowGenerator.train = train\n",
        "WindowGenerator.val = val\n",
        "WindowGenerator.test = test\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37XlQOkAd3rF",
        "outputId": "324d4428-4d37-4dce-917a-3558a4788e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "#using WindowGenerator to define input and label windows\n",
        "#for temperature\n",
        "CONV_WIDTH = 48\n",
        "conv_window = WindowGenerator(\n",
        "    input_width=CONV_WIDTH,\n",
        "    label_width=1,\n",
        "    shift=1,\n",
        "    label_columns=['T (degC)'])\n",
        "\n",
        "conv_window"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Total window size: 49\n",
              "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
              " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47]\n",
              "Label indices: [48]\n",
              "Label column name(s): ['T (degC)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtu4OuV5ZevR",
        "outputId": "ae666621-3d30-41c7-90f9-cb9a19fea262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "conv_window.train\n",
        "for example_inputs, example_labels in conv_window.train.take(1):\n",
        "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
        "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs shape (batch, time, features): (64, 48, 19)\n",
            "Labels shape (batch, time, features): (64, 1, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDi5VVGVBraE"
      },
      "source": [
        "###Transformer Functions###"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA8k83K-_saA"
      },
      "source": [
        "###Positionsl Encoding\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(100, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L4Z1elOLQm0"
      },
      "source": [
        "###Scaled Dot Product\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v):\n",
        "\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
        "  \n",
        "  \n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  \n",
        "\n",
        "  return output, attention_weights\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2mb-aDIMrnr"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  \n",
        "    output = self.dense(concat_attention)  \n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a7_tXa7M792"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MXAOXEbM8fS"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhLmv18cNDxj"
      },
      "source": [
        "#Transformer encoding component\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
        "               maximum_position_encoding, rate):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # There are no embeddings - Just the input added to the positional encoding.\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR9s7ckINKjL"
      },
      "source": [
        "class Transformer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers=4, d_model=48, num_heads=4, dff=16,  \n",
        "               pe_input=100, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, pe_input, rate)\n",
        "\n",
        "  def call(self, inp, training):\n",
        "\n",
        "    enc_output = self.encoder(inp, training)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "    \n",
        "    return enc_output"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7AZnCOo_1sf"
      },
      "source": [
        "# Model Skeleton\n",
        "\n",
        "MAX_EPOCHS = 20\n",
        "\n",
        "def compile_and_fit(model, window, patience=5):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "  model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                optimizer=tf.optimizers.Adam(),\n",
        "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "\n",
        "  history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
        "                      validation_data=window.val,\n",
        "                      callbacks=[early_stopping])\n",
        "  return history, model\n",
        "\n",
        "val_performance = {}\n",
        "performance = {}"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SVoDO4wYdPg",
        "outputId": "9e825425-7d40-449a-a219-29860a149c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "transformer_model = tf.keras.models.Sequential([\n",
        "    tf.keras.Input(shape=(48, 19)), \n",
        "    tf.keras.layers.Dense(48), \n",
        "    Transformer(),\n",
        "    #tf.keras.layers.\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    # Shape => [batch, time, features]\n",
        "    tf.keras.layers.Dense(units=1)  \n",
        "])\n",
        "\n",
        "\n",
        "MAX_EPOCHS = 20\n",
        "\n",
        "transformer_model.compile(loss=tf.losses.MeanSquaredError(),\n",
        "                optimizer=tf.optimizers.Adam(),\n",
        "                metrics=[tf.metrics.MeanAbsoluteError()])\n",
        "transformer_model.summary()\n",
        "\n",
        "history = transformer_model.fit(conv_window.train, epochs=MAX_EPOCHS,\n",
        "                      validation_data=conv_window.val)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 48, 48)            960       \n",
            "_________________________________________________________________\n",
            "transformer (Transformer)    (None, 48, 48)            44800     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 48)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 49        \n",
            "=================================================================\n",
            "Total params: 45,809\n",
            "Trainable params: 45,809\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "766/766 [==============================] - 20s 26ms/step - loss: 0.0617 - mean_absolute_error: 0.1786 - val_loss: 0.1504 - val_mean_absolute_error: 0.3682\n",
            "Epoch 2/20\n",
            "766/766 [==============================] - 20s 26ms/step - loss: 0.0216 - mean_absolute_error: 0.1130 - val_loss: 0.0594 - val_mean_absolute_error: 0.2029\n",
            "Epoch 3/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0174 - mean_absolute_error: 0.1011 - val_loss: 0.0315 - val_mean_absolute_error: 0.1479\n",
            "Epoch 4/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0154 - mean_absolute_error: 0.0946 - val_loss: 0.0167 - val_mean_absolute_error: 0.1036\n",
            "Epoch 5/20\n",
            "766/766 [==============================] - 20s 26ms/step - loss: 0.0143 - mean_absolute_error: 0.0904 - val_loss: 0.0118 - val_mean_absolute_error: 0.0850\n",
            "Epoch 6/20\n",
            "766/766 [==============================] - 20s 26ms/step - loss: 0.0133 - mean_absolute_error: 0.0870 - val_loss: 0.0137 - val_mean_absolute_error: 0.0905\n",
            "Epoch 7/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0124 - mean_absolute_error: 0.0839 - val_loss: 0.0140 - val_mean_absolute_error: 0.0853\n",
            "Epoch 8/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0119 - mean_absolute_error: 0.0818 - val_loss: 0.0108 - val_mean_absolute_error: 0.0787\n",
            "Epoch 9/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0116 - mean_absolute_error: 0.0804 - val_loss: 0.0158 - val_mean_absolute_error: 0.0861\n",
            "Epoch 10/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0112 - mean_absolute_error: 0.0790 - val_loss: 0.0103 - val_mean_absolute_error: 0.0685\n",
            "Epoch 11/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0111 - mean_absolute_error: 0.0788 - val_loss: 0.0138 - val_mean_absolute_error: 0.0849\n",
            "Epoch 12/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0106 - mean_absolute_error: 0.0767 - val_loss: 0.0166 - val_mean_absolute_error: 0.1038\n",
            "Epoch 13/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0106 - mean_absolute_error: 0.0765 - val_loss: 0.0119 - val_mean_absolute_error: 0.0778\n",
            "Epoch 14/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0102 - mean_absolute_error: 0.0754 - val_loss: 0.0092 - val_mean_absolute_error: 0.0677\n",
            "Epoch 15/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0100 - mean_absolute_error: 0.0745 - val_loss: 0.0104 - val_mean_absolute_error: 0.0757\n",
            "Epoch 16/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0099 - mean_absolute_error: 0.0738 - val_loss: 0.0122 - val_mean_absolute_error: 0.0836\n",
            "Epoch 17/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0095 - mean_absolute_error: 0.0723 - val_loss: 0.0089 - val_mean_absolute_error: 0.0705\n",
            "Epoch 18/20\n",
            "766/766 [==============================] - 20s 26ms/step - loss: 0.0097 - mean_absolute_error: 0.0728 - val_loss: 0.0141 - val_mean_absolute_error: 0.0847\n",
            "Epoch 19/20\n",
            "766/766 [==============================] - 19s 24ms/step - loss: 0.0093 - mean_absolute_error: 0.0714 - val_loss: 0.0149 - val_mean_absolute_error: 0.0998\n",
            "Epoch 20/20\n",
            "766/766 [==============================] - 19s 25ms/step - loss: 0.0095 - mean_absolute_error: 0.0719 - val_loss: 0.0124 - val_mean_absolute_error: 0.0832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_EpbOeqeJRX",
        "outputId": "c9ae91ae-4723-49b5-9fbe-0bd0c3d69869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "val_performance['Transformer'] = transformer_model.evaluate(conv_window.val)\n",
        "performance['Transformer'] = transformer_model.evaluate(conv_window.test, verbose=0)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "219/219 [==============================] - 3s 13ms/step - loss: 0.0124 - mean_absolute_error: 0.0832\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}